Many AI problems can be seen as \textbf{constraint satisfaction problems}, in other words, find a state of the problem that meets a given set of constraints. In fact, in some
cases, we cannot take decisions freely but we have to respect some given rules. \vspace{3.5pt}

We have already seen one example of constraint satisfaction: the eight queens problem. The aim of the 8 queens problem consists in placing eight queens in a chessboard in order
to avoid mutual attacks. \vspace{3.5pt}

For solving this kind of problem, several model formulations can be used:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{}
    \item $1^{st}$ Model.
    \begin{itemize}[nosep]
        \renewcommand{\labelitemii}{-}
        \item The domain of the possible values is: $[0, 1]$.
        \item Every cell $(i,j)$ of the $N \times N$ board is associated with a binary value $x_{i,j}$.
        \item If the variable $x_{i,j}$ is equal to $1$, it means that this position is currently assigned to a queen; if the value is $0$, the cell is free. 
        \item The \textbf{constraint} is the sum in any possible row, column or diagonal cannot be greater than $1$.
    \end{itemize} 
    \item $2^{nd}$ Model.
    \begin{itemize}[nosep]
        \renewcommand{\labelitemii}{-}
        \item The $N$ queens are represented by $N$ variables: $x_1, x_2, ..., x_N$.
        \item The domain of the variables is the set of integers between $1$ and $N$, which correspond to the row occupied.
        \item The subscript of the variable $x_j$ refers to the column occupied by the corresponding queen. For instance, the ordering $\langle 1, 6, 2, 5, 7, 5, 8, 3\rangle$ determines that the queen in the first column $x_1$ is situated in the first row, the queen in the second column $x_2$ is in the sixth row and so on.
    \end{itemize} 
    \item This model requires four different constraints:
    \begin{itemize}[nosep]
        \renewcommand{\labelitemii}{-}
        \item $1 \le x_i \le N$ for $1 \le i \le N$.
        \item $x_i \neq x_j$ for $1 \le i < j \le N$.
        \item $x_i \neq x_j + (j - i)$ for $1 \le i < j \le N$.
        \item $x_i \neq x_j - (j - i)$ for $1 \le i < j \le N$.
    \end{itemize} \vspace{3.5pt}

    The first constraint requires that the values of the variables of the problem are integers between $1$ and $N$, where those values determine in which rows 
    the queens are located. The next three constraints define relationships between the variables and, in particular, involving two variables at the time. \vspace{3.5pt}

    In addition, we can improve the description saying: the \textbf{second constraint} requires that two queens do not share the same line. The \textbf{third} and the \textbf{fourth constraint}
    concern the positions on the two diagonals starting from the initial box.
\end{itemize} 

An other example of constraint problem is \textbf{scheduling}: assign tasks to resources at given time. Another one is the \textbf{map coloring} problem: we need to color portions of a map,
characterized by a number, in such a way that two contiguos regions are colored with different colors. \vspace{3.5pt}
\begin{center}
    \includegraphics[width=0.5\textwidth]{img/img34.png}    
\end{center} \vspace{3.5pt}

What do all these problems have in common? Every \textbf{Constraint Satisfaction Problem} is defined on a finite set of variables:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item $X_1, X_2, ..., X_n$ decisions that we have to take.
    \item $D_1, D_2, ..., D_n$ domains of the possible values.
    \item A set of constraints.
\end{itemize}

A constraint $c(X_{i1}, X_{i2}, ..., X_{ik})$ between $K$ variables is a subset of the \textbf{cartesian product} $D_{i1} \times D_{i2} \times ... \times D_{ik}$ that 
specifies which values of the variables are compatible with each other.
\begin{example}
    i.e. Cartesian product of domains. \vspace{3.5pt}

    Given \vspace{3.5pt}

    \begin{center}
        $D_1 = \{1, 2, 3\}$ and $D_2 = \{1, 2, 3\}$
    \end{center} \vspace{3.5pt}

    the cartesian product of the two domains is a set of tuples containing the combination of the domains' values. \vspace{3.5pt}

    \begin{center}
        $Result = \{(1,1), (1,2), (2,1), (1,3), (3,1), ...\}$
    \end{center} \vspace{3.5pt}

    A possible constraint would be: the $X$ values must be greater than the $Y$ values, $X > Y$. Therefore, we have to select all the tuples that have the first element greater than the second one.
\end{example}

\textbf{CSPs} can be solved through \textbf{search strategies}, but we have to define a \textit{"smart way"} to search the solution inside the \textbf{search space}. In an 
n-variables problem in which all the domains have the same cardinality $d$, the number of leaves of the search tree is equal to $d^n$. We can use \textit{constraints}
to remove from the search space (the set of all the sequences of actions) useless states that are obviously wrong. \vspace{3.5pt}

If we do an exhaustively search the problem becomes \textbf{exponential}, we have to erase paths that are in conflict with the set of constraints. There are two possible 
approaches:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item \textbf{Propagation Algorithms}. \\ A \textit{priori pruning} technique, that uses constraints between the variables of the problem to \textbf{reduce the search space}.
    \item \textbf{Consistency Tecniques}. \\ Based on the propagation of constraints in order to derive a \textbf{simpler problem} than the original one.
\end{itemize}  

We start from propagation algorithms. The main idea about these techniques is to check which are the future constraints according to the current assignment already done.
Let's suppose we have a list of variables described as follows:
\begin{center}
    $X_1, X_2, ..., X_{i-1}, X_i, X_{i+1}, ..., X_N$
\end{center}
where:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item $X_1, X_2, ..., X_{i-1}$ are variables already assigned.
    \item $X_i$ is the variable taken into account for the assignment.
    \item $X_{i+1}, ..., X_{N}$ are free variables, waiting for an assignment. 
\end{itemize}

As soon as we have done the \textbf{current assignment} we remove all the failures from that assignment. We can remove all the possible failures by:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item \textbf{Forward checking}. \\ After each assignment of variable $X_i$, the forward checking propagates all constraints involving $X_i$ and all the future variables. If we end up with an \textbf{empty domain} (no values left for assignment still available in order to satisfy the set of constraints) the forward checking fails and backtracks.
    \item \textbf{Look ahead}. \\ Look ahead checks the existence of values that are compatible with the constraints about only the non-instantiated variables. It does this kind of reasoning: \textit{while assigning $X_i$, there exists a support value of $X_i$ in $X_{i+1}, ..., X_N$ such that the assignment already done does not cause any violation?}
\end{itemize}

In addition, look ahead strategy is divided into: 
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item \textbf{Partial Look Ahead}. \\ For each value in the domain of $X_i$ it checks if in the domain of not yet instantiated variables $X_{i+1}, ..., X_N$ there is a value compatible with it. 
    \item \textbf{Full Look Ahead}. \\ For each value in the domain of $X_i$ it checks if in the domain of not yet instantiated variables $X_{k+1}, ..., X_{i-1}$, $X_{i+1}, ..., X_N$ there is a value compatible with it.
\end{itemize}
\begin{example}
    i.e. Look ahead examples. \vspace{3.5pt}

    Given this set of variables \vspace{3.5pt}
    \begin{center}
        $X_0$ $X_1$ $X_2$ $X_3$
    \end{center} \vspace{3.5pt}
    with domains $X_1, X_2, X_3 :: [1,2,3]$ and given the constraint \vspace{3.5pt}
    \begin{center}
        $X_0 < X_1 < X_2 < X_3$
    \end{center} \vspace{3.5pt}
    find out all the compatible assignments using the two strategies of look ahead. \vspace{3.5pt}

    Starting from \textbf{partial look ahead}, for each value in the domain of $X_1$ it checks if there exists at least one value in the domain of $X_2$ and at least one value inside the domain of the variable $X_3$. If it does not exist we delete the value from the domain of $X_1$. For each value in the domain of $X_2$ variable it checks if there exists at least one value in the domain of variable $X_3$ compatible. \vspace{3.5pt}
    \begin{center}
        \begin{tabular}{l}
                $X_1 :: [1,2]$ \\
                $X_2 :: [1,2]$ \\ 
                $X_3 :: [1,2,3]$
        \end{tabular}
    \end{center} \vspace{3.5pt}

    \textbf{Full look ahead}, beside the checks performed by partial look ahead, checks also for each value that belongs to the domain of variable $X_2$ if there is at least a value compatible inside the domain of variable $X_1$ and for each value in $X_2$ checks if there exists one value compatible in the domain of the variable $X_3$. Finally, it does a check for each value of $X_3$ if there exists a compatible value either in the domain of the variables $X_1$ and $X_2$. \vspace{3.5pt}
    \begin{center}
        \begin{tabular}{l}
                $X_1 :: [1,2]$ \\
                $X_2 :: [2]$ \\ 
                $X_3 :: [3]$
        \end{tabular}
    \end{center} \vspace{3.5pt}
\end{example}

The order of variable selection and value selection can affect the efficiency of CSP resolution. The \textbf{heuristics} can then act on these two degrees of freedom to try to
ensure the achievement of a good solution in a reasonable time for even the most complex problems. The heuristics can be classified into:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item \textbf{Variable selection heuristics}. \\ It answers to the query: \textit{which should be the next variable to instantiate}? The most commonly heuristics are the \textbf{first-fail} (selects always the variable with the smallest domain) and the \textbf{most-constrained principle} (chooses the variable appearing in the largest number of constraints). Usually, the most used is the \textbf{first-fail heuristic}.
    \item \textbf{Value selection}. It asks: \textit{what value to assign to the selected variable}? There are no formalized rules about value selection; usually, we follow the assignment of the most promising value.
\end{itemize}

A further classification is as follows:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item \textbf{Static heuristics}. \\ Before the search process, we have already a given order of the variables.
    \item \textbf{Dynamic heuristics}. \\ Every time an assignment is done, so the domains are just changed, we check the variable with the smallest domain.
\end{itemize}

A CSP can be seen as a \textbf{constraint graph}, where nodes are variables and the links connect variables that partecipate to a constraint. The simplest kind of CSP involves
variables that have a finite domain. The \textit{8 queens problem} is one of them, where the variables $Q_1, ..., Q_8$ are the positions of each queen in columns $1, ..., 8$ and each variable
has the domain $D_i = \{1,2,3,4,5,6,7,8\}$. \vspace{3.5pt}

In CSP an algorithm can perform the search process or do a constraint propagation: reducing the number of legal moves using constraints. It's more efficient than generate and create 
the whole tree and then remove parts of it that does not match the set of constraints. \vspace{3.5pt}

The other key idea about CSPs is the notion of \textbf{local consistency}. We enforce local consistency inside the constraint graph such that inconsistent values are deleted 
throughout the graph. To guarantee local consistency the graph should respect:
\begin{itemize}[nosep]
    \renewcommand{\labelitemi}{-}
    \item \textbf{Node consistency}. \\ A single variable (that matches a node inside the graph) is \textbf{node-consistent} if all the values in the variable's domain satisfy the variable's unary constraints. For instance, if we assume a variant of the map coloring problem, such as the first region cannot be colored \textit{green} and the starting domain is \textit{\{green, blue, red\}}, we can make the node consistent simply eliminating the \textit{green} color from the variable's domain.
    \item \textbf{Arc consistency}. \\ In this case, we handle the binary constraint between variables. A variable in a CSP is \textbf{arc-consistent} if every value in its domain satisfies the variable's binary constraints. More formally, taking two variables $X_i$ and $X_j$, we said \textit{$X_i$ is arc-consistent with $X_j$} if: \vspace{3.5pt}
    \begin{center}
        $\forall v \in D_i$ $\exists z \in D_j$ s.t. the binary constraint $(X_i,X_j)$ is satisfied.
    \end{center} \vspace{3.5pt}
\end{itemize}

The consistency is obtained by the iteration of the \textbf{full look ahead} approach. The arc consistency, as for node consistency, is applied before the search process and 
while the solver is searching. By the way, reaching local consistency does not imply \textbf{feasible solutions}. For this main reason, we can describe a higher level of 
consistency, like \textbf{path consistency}. Path consistency reinforces the binary constraints by using implicit constraints that are inferred by looking at triples of 
variables. \vspace{3.5pt} 

Given a binary constraint $(X_i, X_j)$ and a third variable $X_m$, $(X_i, X_j)$ is \textbf{path consistent} with respect to $X_m$, if for every assignment $X_i$ and $X_j$ that makes consistent the relation $(X_i,X_j)$, there exists
an assignment $X_m$ such that $(X_i,X_m)$ and $(X_j,X_m)$ are satisfied. More formally: \vspace{3.5pt}
\begin{center}
    $\forall v \in D_i$ and $\forall z \in D_j$ that makes $(X_i, X_j)$ consistent, $\exists x \in D_m$ s.t. the constraints $(X_i,X_m)$ and $(X_j,X_m)$ are satisfied
\end{center}